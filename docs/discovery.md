# Project Brief: Python FastAPI API Gateway (Async & Streaming)

## Overview  
This project involves developing a **Python-based API Gateway** using **FastAPI** to aggregate and manage requests to multiple upstream APIs (e.g. different third-party AI/LLM providers). The gateway will be **containerized with Docker** for easy deployment and scalability. Key features include high-concurrency handling via FastAPI’s asynchronous capabilities, **API key-based authentication** for all clients, support for **streaming responses** (both Server-Sent Events and raw HTTP streaming), and modular routing (separate endpoints per provider and custom “project” endpoints). Additionally, the gateway will integrate with **Arize Phoenix** for observability – capturing distributed traces and prompt logs for each request – and be designed to accommodate extensible tooling or plugins (including a possible **Model Context Protocol (MCP)** integration). The goal is to provide a unified, secure, and efficient entry point for clients to access various AI services, with robust monitoring and the flexibility to extend functionality.

## Objectives  
- **Unified API Access:** Provide a single gateway that exposes multiple upstream AI services via a consistent interface. Each upstream provider’s API will be accessible through a dedicated endpoint on the gateway, simplifying client integrations.  
- **High Performance & Scalability:** Leverage FastAPI’s speed and asynchronous I/O to handle numerous concurrent requests with low latency ([Building a API Gateway with FastAPI: Rate Limiting, Authentication, and Observability | by Bryan Antoine | Medium](https://medium.com/@b.antoine.se/building-a-api-gateway-with-fastapi-rate-limiting-authentication-and-observability-7a0f658f14a8#:~:text=Why%20FastAPI%20for%20API%20Gateways%3F)). The gateway should efficiently multiplex I/O-bound calls to external APIs, achieving throughput comparable to lower-level languages (FastAPI async performance can be on par with Go and faster than many Node.js frameworks ([Concurrency and async / await - FastAPI](https://fastapi.tiangolo.com/async/#:~:text=higher%20performance%20than%20most%20of,all%20thanks%20to%20Starlette))). Containerization will facilitate horizontal scaling (running multiple instances behind a load balancer) to meet demand.  
- **Secure Access Control:** Enforce API key authentication on all incoming requests. Only clients providing a valid key (e.g. via an HTTP header) can use the gateway, preventing unauthorized access. This ensures the gateway can serve multiple clients or projects safely by isolating their access with credentials.  
- **Streaming Responses:** Enable real-time data streaming for clients. The gateway will support Server-Sent Events (for event-streaming to browsers or SSE-compatible clients) as well as raw HTTP chunked streaming for other clients. This allows use cases like live AI model outputs or progress updates to be delivered incrementally, without requiring clients to poll or wait for the full response.  
- **Project-Specific Endpoints:** Allow creation of custom “project” endpoints that implement bespoke logic or workflows for particular projects. These endpoints may combine multiple upstream calls or enforce project-specific business rules (e.g. a custom prompt template or post-processing step) before returning a result. The objective is to make the gateway adaptable to different application needs without affecting the core provider endpoints.  
- **Observability & Tracing:** Integrate tracing and logging such that each API request through the gateway is tracked. Using Arize Phoenix, the system should record detailed traces of the request’s lifecycle (including upstream calls, timings, prompt details, etc.) ([Overview: Tracing | Phoenix](https://docs.arize.com/phoenix/tracing/llm-traces#:~:text=,to%20optimize%20performance%20and%20responsiveness)) ([Overview: Tracing | Phoenix](https://docs.arize.com/phoenix/tracing/llm-traces#:~:text=temperature%20and%20system%20prompts%2C%20to,ensure%20optimal%20configuration%20and%20debugging)). This provides insight into performance bottlenecks, usage patterns, and helps with debugging or optimizing prompts. Prompt inputs and relevant metadata should be captured (“loaded”) into the tracing system for later analysis and evaluation.  
- **Extensibility:** Design the gateway to be easily extensible. Developers should be able to add new upstream providers or new integration tools with minimal changes. For example, if a new observability tool or plugin (such as one conforming to an emerging **Model Context Protocol (MCP)** standard) needs to be integrated, the gateway’s architecture should support plugging it in without a complete rewrite. This future-proofs the system as the AI tooling ecosystem evolves.

## Architecture  
**Overall Design:** The API Gateway will be implemented as a **FastAPI application** running under an ASGI server (Uvicorn) inside a Docker container. FastAPI is chosen for its modern design and excellent support for async operations, which are ideal for high-concurrency API gateways ([Building a API Gateway with FastAPI: Rate Limiting, Authentication, and Observability | by Bryan Antoine | Medium](https://medium.com/@b.antoine.se/building-a-api-gateway-with-fastapi-rate-limiting-authentication-and-observability-7a0f658f14a8#:~:text=Why%20FastAPI%20for%20API%20Gateways%3F)). The gateway will be stateless – it does not permanently store data itself – and primarily acts as a **reverse proxy and orchestrator** for upstream API calls with added cross-cutting concerns (auth, logging, etc.). The architecture emphasizes a modular structure: distinct components handle routing, authentication, upstream communication, streaming, and integrations, all interacting within the FastAPI app. 

**Endpoint Routing:** We will use FastAPI’s **APIRouter** or similar mechanisms to compartmentalize endpoints: 

- *Provider Endpoints:* Each external API provider gets a dedicated route prefix (for example, `/provider1/...`, `/provider2/...`). Under these prefixes, the gateway defines endpoints corresponding to that provider’s functionality (e.g., completion, search, etc., depending on the provider’s API). This separation ensures clarity and prevents overlap. It also allows provider-specific logic (like request/response translation) to be encapsulated within that router module. New providers can be added by registering a new router with its prefix – this pluggable design makes the gateway **modular**.  

- *Project Endpoints:* In addition to provider-based routing, the gateway will have special routes for certain projects (e.g., `/projects/projectX/...`). These project endpoints might implement custom workflows – for instance, an endpoint that internally calls one or multiple upstream providers in sequence or applies custom business logic for that project. They serve as **composed operations** tailored to project requirements. By isolating them under a `/projects` namespace (or similar), we keep the main API organized. The architecture might implement each project’s logic in a separate module or router for maintainability. This way, enabling or updating a project-specific feature doesn’t impact other parts of the system.  

**Authentication Layer:** A global API key authentication mechanism will be enforced for (almost) all routes. We will implement this using FastAPI dependencies or middleware. For example, we can use `fastapi.security.APIKeyHeader` to define a required header (say, `X-API-Key`) for all protected endpoints ([Security Tools - FastAPI](https://fastapi.tiangolo.com/reference/security/#:~:text=API%20key%20authentication%20using%20a,header)). Each request will be checked for a valid key against a store of allowed keys (this could be an in-memory list, a database table, or an environment-provided list of keys). If the key is missing or invalid, the request is rejected with an appropriate HTTP 401/403 error. By integrating the API key check at a dependency level, it can automatically apply to all endpoints without duplicating code. (Public endpoints like health checks can be exempted.) This approach cleanly integrates with FastAPI’s documentation as well – the OpenAPI spec can document the API key header requirement automatically ([Security Tools - FastAPI](https://fastapi.tiangolo.com/reference/security/#:~:text=API%20key%20authentication%20using%20a,header)). 

**Asynchronous Request Handling:** All endpoint logic will use `async def` to maximize concurrency. When a request comes in, the FastAPI app will spawn an asynchronous handler that can perform awaiting calls to upstream APIs. Because the operations (calls to external services) are I/O-bound, using `asyncio` allows the server to handle other requests while waiting for the external API to respond. This non-blocking design enables high throughput under concurrent load. In effect, the gateway can handle many simultaneous client connections and API calls with fewer threads/processes, yielding performance benefits (as noted, FastAPI’s async approach yields performance competitive with Node.js and Go servers ([Concurrency and async / await - FastAPI](https://fastapi.tiangolo.com/async/#:~:text=higher%20performance%20than%20most%20of,all%20thanks%20to%20Starlette))). For CPU-bound tasks (if any), we would offload to background tasks or worker threads to avoid blocking the event loop, but primarily the gateway’s job is I/O forwarding. 

**Upstream API Clients:** For each provider, the gateway will include a client integration. This could be implemented with an async HTTP client library (such as `httpx` or `aiohttp`) or the provider’s SDK if one exists (ensuring it supports async). These client modules know how to call the provider’s endpoints (URL, required auth credentials, request format) and may also transform the provider’s response into a normalized format for the client. For example, if Provider A expects a JSON payload with fields `prompt` and `max_tokens`, while Provider B expects `query` and `length`, the gateway’s Provider A route will accept our standardized field names and convert as needed before calling Provider A’s API. We will store configuration like API base URLs and provider API keys in environment variables or a config file (never hard-coded) and have the client modules use them. The architecture also anticipates that some providers might support **streaming** responses – in such cases, the client module will handle reading the stream incrementally and passing data back to our FastAPI response generator. Each provider integration will be designed as a self-contained component, making it easy to update or add providers by following the same interface. 

**Streaming Support:** A critical architectural aspect is supporting streaming outputs. There are two streaming modes to implement: 

- *Server-Sent Events (SSE):* SSE will be used when the client specifically wants an event stream (commonly for browser clients using `EventSource`). Architecturally, this means our endpoint will not return a normal JSON response but rather a streaming response with content type `text/event-stream`. In FastAPI/Starlette, we can accomplish this by returning a `StreamingResponse` or using an event stream utility that yields data periodically. For SSE, the response data must follow the SSE format (i.e. text lines starting with “data: ” and terminated with double newlines, optionally with “event:” or other fields). The gateway will likely provide helper functions to format upstream partial results into SSE-compliant messages. For example, if an upstream LLM sends partial completion texts, we wrap each piece as `data: ...\n\n` before yielding. SSE connections remain open until the process is complete, then we send a terminator (or simply close). Internally, the architecture could dedicate a small coroutine to listen to the upstream and push events to the client. SSE is beneficial for one-way real-time updates without the overhead of full WebSockets ([Server-Sent Events with Python FastAPI | by Nanda Gopal Pattanayak | Medium](https://medium.com/@nandagopal05/server-sent-events-with-python-fastapi-f1960e0c8e4b#:~:text=SSE%20is%20a%20very%20good,to%20achieving%20the%20best%20results)), and is ideal for pushing AI model token streams. 

- *Raw HTTP Streaming:* In some cases, clients might prefer or require a raw HTTP stream (not SSE). This could be the case for command-line tools or SDKs that want chunked JSON responses. The architecture will allow an endpoint to yield raw chunks (e.g., partial JSON objects or text) without SSE framing. Technically, this is similar to SSE (both use HTTP/1.1 chunked transfer under the hood), but the content type might be `application/json` or others, and the client must parse incrementally. We will ensure the FastAPI route can detect or be configured for which mode to use – possibly via a query parameter like `?stream_mode=sse` or via the `Accept` header (`text/event-stream` vs `application/json`). Internally, if an upstream API call returns an async iterator (as some OpenAI SDKs do for streamed completions) or if using a callback mechanism, the gateway will iterate through results and stream out. **No blocking waits:** The implementation will *yield data as soon as it’s available from upstream*, so end-to-end latency is minimized. 

- *Connection Management:* Because streaming responses keep connections open, we must consider scalability. Each open stream consumes some resources, so the design might include limits (e.g., max concurrent streams per process) to avoid overload. Also, to work correctly behind proxies or load balancers, we’ll set appropriate headers to disable buffering. For instance, we’ll include `Cache-Control: no-cache` and `X-Accel-Buffering: no` in SSE responses so that an Nginx proxy doesn’t buffer the stream ([For Server-Sent Events (SSE) what Nginx proxy configuration is appropriate? - Server Fault](https://serverfault.com/questions/801628/for-server-sent-events-sse-what-nginx-proxy-configuration-is-appropriate#:~:text=However%2C%20instead%20of%20turning%20,fastcgi)) ([For Server-Sent Events (SSE) what Nginx proxy configuration is appropriate? - Server Fault](https://serverfault.com/questions/801628/for-server-sent-events-sse-what-nginx-proxy-configuration-is-appropriate#:~:text=Content,no)). This ensures clients receive events in real time. We’ll also likely implement a heartbeat or ping event for SSE to keep the connection alive if the upstream has infrequent data, as proxies might drop an “idle” connection otherwise ([For Server-Sent Events (SSE) what Nginx proxy configuration is appropriate? - Server Fault](https://serverfault.com/questions/801628/for-server-sent-events-sse-what-nginx-proxy-configuration-is-appropriate#:~:text=Content,no)). These details will be part of deployment tuning (see Deployment Considerations). 

**Response Handling and Transformation:** The gateway’s job isn’t just to forward bytes; it may also unify or enrich responses. The architecture will include logic to standardize error responses (e.g., if an upstream returns a 500 or a specific error structure, the gateway can catch that and return an intelligible JSON error to the client). Similarly, if project endpoints combine multiple calls, they will compose a single response from them. Consistent JSON schemas for responses can be defined for project endpoints. For provider endpoints, we might choose to pass through the provider’s response mostly as-is (to minimize overhead and because the client may already expect the provider’s format), or do minimal transformation (like always enveloping it in a common structure or adding metadata). These decisions will be documented per endpoint. All transformations will happen in-memory streaming fashion when possible (for streaming, we might transform each chunk on the fly). 

**Logging & Monitoring Components:** Within the application, we will set up logging (using Python’s `logging` module or an observability SDK). Each request will log an entry (method, path, client id (derived from API key or a token), response status, and latency). For streaming responses, we might log when the stream started and when it finished, including total duration and bytes streamed. These logs will go to stdout (and picked up by Docker logging) or to a logging service as configured. In addition, we integrate **distributed tracing** (see Integration Details below) to capture request spans. The architecture might incorporate a middleware that starts a trace span for each incoming HTTP request and automatically closes it on completion. We will also use hooks in the code to annotate spans with important events (e.g., “called ProviderA API”, “received response of size N bytes”, etc.). 

**Observability Integration:** A major architectural focus is built-in observability through Arize Phoenix (via OpenTelemetry). At app startup, we will initialize the OpenTelemetry tracer (using Arize’s `phoenix.otel` or the standard OTel SDK). This sets up a global **TracerProvider** and configures an exporter to send trace data to the Arize Phoenix backend ([Setup using Phoenix OTEL | Phoenix](https://docs.arize.com/phoenix/tracing/how-to-tracing/setup-tracing/setup-using-phoenix-otel#:~:text=Quickstart%3A%20)) ([Setup using Phoenix OTEL | Phoenix](https://docs.arize.com/phoenix/tracing/how-to-tracing/setup-tracing/setup-using-phoenix-otel#:~:text=Phoenix%20Authentication)). The tracer will be used throughout the request lifecycle. For example, when a request comes to `/provider1/completions` endpoint, we can start a new trace span named “Provider1CompletionRequest”. As we process the request, we attach attributes to the span such as the provider name, model or endpoint being called, maybe the project ID (if applicable), and a request ID. Crucially, **prompt data** (if the request contains a prompt or query) will be added to the trace. We might store the prompt text or a hashed reference of it as a span attribute (e.g., `llm.prompt`), and any parameters like `max_tokens`, `temperature` as well ([Overview: Tracing | Phoenix](https://docs.arize.com/phoenix/tracing/llm-traces#:~:text=,ensure%20optimal%20configuration%20and%20debugging)). Then, when the gateway calls the upstream API, that external call can be traced as a child span – if we use an instrumented HTTP client, it could automatically create spans for outgoing HTTP calls. This nesting will show up in Phoenix, giving a timeline of the request: received -> called upstream -> got response -> sent to client, etc. Once the response is completed (or stream ended), we end the span and the trace is sent to Phoenix’s collector. This allows Phoenix to record metrics like latency of the external API call, the total time we spent, how many tokens were in the prompt/response (if we capture that), etc. Over time, this trace data in Phoenix can be analyzed to find slow spots or errors. For instance, Phoenix can help identify a slow LLM invocation or high token usage for certain prompts ([Overview: Tracing | Phoenix](https://docs.arize.com/phoenix/tracing/llm-traces#:~:text=,to%20optimize%20performance%20and%20responsiveness)). The architecture, therefore, includes an **OpenTelemetry instrumentation layer** that’s largely transparent to the business logic – developers writing endpoint code will just include some tracing calls or decorators to log important data, and the rest is handled by the OTel SDK.

**Extensibility & Plugin System:** To meet the requirement of extensible tooling, the architecture will be designed with clear extension points. This could mean defining abstract classes or interfaces for certain components. For example, we might define an interface for “UpstreamProviderClient” which all provider client classes must implement (with methods like `send_request(request_data) -> response_data`). New providers then simply implement this interface and register themselves. Similarly, for **project-specific logic**, we can allow projects to have their own module that the gateway discovers via config. If needed, a plugin system can be implemented (perhaps using Python entry points or a simple factory pattern) to load additional functionalities. One key envisaged extension is the **Model Context Protocol (MCP)** integration. MCP is an emerging open standard (pioneered by Anthropic) aimed at connecting LLMs to external data/tools in a unified way ([Model Context Protocol - Arize AI](https://arize.com/blog/model-context-protocol/#:~:text=MCP%20is%20a%20universal%2C%20open,AI%20and%20any%20external%20functionality)). Though details are to be clarified, we assume we may need to integrate an MCP *server* or client. Architecturally, this could be done by **mounting an additional FastAPI app or router dedicated to MCP** communications. For example, an MCP server could run as part of this application, listening on a particular route (the Ragie.ai example demonstrates embedding an MCP server and exposing it via SSE within a FastAPI app ([Building a Server-Sent Events (SSE) MCP Server with FastAPI](https://www.ragie.ai/blog/building-a-server-sent-events-sse-mcp-server-with-fastapi#:~:text=The%20Model%20Context%20Protocol%20,support%20in%20your%20FastAPI%20application))). Our gateway could include an “MCP integration module” that, if enabled, spins up the necessary background tasks or routes for MCP. Because MCP often involves streaming interactions (the standard suggests using SSE for connecting AI models with tools), this fits well with our streaming-ready architecture. In summary, the system is built so that new routes, new providers, or even new cross-cutting concerns (like another tracing system, analytics collection, etc.) can be added with minimal changes to existing code. Loose coupling and clear module boundaries are core architectural principles here.

## Functional Requirements  

**1. Multi-Provider API Gateway:** The system must support multiple upstream API providers and expose each via a distinct endpoint namespace. For each configured provider:  
- *Dedicated Endpoints:* Provide a set of HTTP endpoints that map to that provider’s API operations. For example, if Provider A offers a “completion” API, the gateway might expose `POST /providerA/completions` for client requests, which internally calls Provider A’s actual API. Each provider’s endpoints may accept slightly different request payloads as needed but will follow a consistent style. Crucially, calls meant for one provider should not be routed to another; isolation by endpoint prefix or path is required.  
- *Request Handling:* The gateway will parse and validate incoming requests for required fields. It should translate or augment the request as needed for the upstream. E.g., include the provider’s API key or credentials (stored securely in the gateway’s environment/config) and adjust any parameter names. After forwarding the request, the gateway receives the response from the provider and relays it back to the client. If the provider’s response is streaming (chunked), the gateway should stream it out to the client in real-time (maintaining the mode as discussed). If the provider’s response is a one-time JSON, the gateway can forward that JSON (possibly filtering out any internal info).  
- *Error Propagation:* In case the upstream provider returns an error (HTTP error status or network failure), the gateway should catch this and translate it into a proper HTTP response for the client. For instance, a 500 from upstream might be returned as a 502 Bad Gateway from our gateway, along with a message. Timeouts should be enforced – if an upstream doesn’t respond within a configured time (say 30s), the gateway should cancel the request and return a timeout error to the client to avoid hanging connections.  
- *Configuration:* It should be easy to configure new providers. The gateway might read a configuration file or environment variables listing the enabled provider integrations, their base URLs, and credentials. Only those configured will be active. This allows deploying the gateway in different environments (e.g., dev, staging, prod) with different sets of upstreams or using dummy upstreams for testing.  

**2. Project-Specific Endpoints and Workflows:** In addition to the provider-centric routes, the gateway will offer custom endpoints associated with specific projects or use-cases:  
- *Custom Logic:* Each project endpoint implements a defined custom behavior. For example, a project endpoint `/projects/acme-summarizer/summarize` might internally call two different providers (one for retrieving data and another for summarization) and then combine the results. Another example: a project might have a tailored prompt that all user queries must be appended to; the project endpoint can do that before calling an upstream LLM. The gateway should allow encoding such logic in code.  
- *Isolation:* Project endpoints are logically separated from core provider endpoints. They may use one or more providers under the hood, but to the client they appear as a single cohesive API tailored to the project. We will document each project endpoint’s contract (input/output) separately.  
- *Ease of Addition:* Adding a new project route should not require touching the low-level framework code. Developers can create a new module (or router) for the project, implement the needed calls internally, and mount it. This is facilitated by the architecture. Also, if a project becomes deprecated, we can remove or disable its routes without affecting others.  
- *Authentication & Authorization:* Project endpoints will also require API keys – potentially, we could enforce that only certain API keys can access certain project endpoints (if we have a mapping of keys to projects). This would be an extension of the auth system: for example, an API key might carry a “project” scope claim. If so, the gateway should verify that and only permit calls to matching project endpoints. This ensures one client/project can’t accidentally call another’s custom API. (If not implemented initially, it’s a consideration for security as projects multiply.)  

**3. API Key Authentication:** Every request to the gateway (except possibly public utility endpoints like health checks) **must include a valid API key** for authentication.  
- *Mechanism:* The client will provide an API key in a header (e.g., `X-API-Key: <token>`). Upon receiving a request, the gateway checks for this header. We’ll use a FastAPI dependency to extract the key easily ([Security Tools - FastAPI](https://fastapi.tiangolo.com/reference/security/#:~:text=API%20key%20authentication%20using%20a,header)). If the key is missing or not recognized, the gateway will respond with HTTP 401 Unauthorized or 403 Forbidden (along with a JSON error message indicating an authentication failure). This check happens before any heavy processing or calling upstream services.  
- *Key Management:* The gateway will maintain a list or database of valid API keys. In a simple setup, this could be a config file or environment variable that lists acceptable keys (or a simple hashed password). In a more advanced setup, we could integrate with an identity service or database. For the scope of this project, a straightforward approach (like an in-memory list for demonstration, or reading from a secure file) is acceptable, with the ability to plug in a more robust solution later.  
- *Differentiation:* The system should ideally be able to identify the client or project based on the API key. For instance, if key “ABC” belongs to Project X, and key “DEF” to Project Y, the gateway might use this information in logging or to restrict access (as noted above). Initially, all keys might have the same privileges, but we lay the groundwork (for example, by associating each key with metadata like “project: X”) for future authorization rules.  
- *No Key, No Access:* Emphasizing security, there should be no way to access the main functionality without a key. This prevents unauthorized usage and also helps us meter usage per key externally if needed. (In the future, we might implement rate limiting per key or usage tracking, but that’s beyond the current requirements.)  
- *Integration with Docs:* If the automatically generated docs (Swagger UI) are exposed, we can integrate the API key authentication there as well. FastAPI can document an APIKeyHeader security scheme so that users know they must provide the header to try out the endpoints ([Security Tools - FastAPI](https://fastapi.tiangolo.com/reference/security/#:~:text=API%20key%20authentication%20using%20a,header)). We will likely hide the docs or protect them in production, but during development this is useful.

**4. Streaming Response Support:** The gateway must support streaming responses in two formats – **Server-Sent Events (SSE)** and **raw HTTP streaming** – allowing clients to receive data progressively.  
- *Server-Sent Events (Event Stream):* When SSE mode is requested, the endpoint will respond with `Content-Type: text/event-stream` and stream a sequence of events. Each event will typically be a line beginning with `data: ` followed by the event data (often JSON), ending with a double newline. The gateway might also send an initial `event: open` or comments to establish the stream (and possibly periodic `: heartbeat` comments to keep the connection alive). SSE is ideal for web clients; for example, a browser app can open an EventSource to our endpoint and get real-time updates. The implementation will use an async generator or similar construct in FastAPI to yield events. We will flush each event as soon as it’s available (no buffering on the application side). SSE has the advantage that the client will automatically reconnect if the connection drops, but for our purposes reconnection logic can be left to the client. **Use case:** Streaming LLM outputs token-by-token to a web UI. SSE is a good fit and avoids heavier protocols like WebSockets when only one-way communication is needed ([Server-Sent Events with Python FastAPI | by Nanda Gopal Pattanayak | Medium](https://medium.com/@nandagopal05/server-sent-events-with-python-fastapi-f1960e0c8e4b#:~:text=SSE%20is%20a%20very%20good,to%20achieving%20the%20best%20results)).  
- *HTTP Chunked Streaming:* In scenarios where SSE format is not desired, the gateway will simply stream chunks of the response. For instance, if the upstream API (like OpenAI) streams chunks of a completion as JSON objects prefixed by `data: ` (as OpenAI’s API does), the gateway could forward those directly with `Content-Type: application/json` (or text) and each chunk separated by the HTTP chunked encoding. The client on the other end might read from the response stream and parse JSON incrementally. This mode might be useful for non-browser clients or where SSE event framing is not needed. The gateway should make it easy to switch between SSE and raw mode. Perhaps a query parameter `?stream=sse` vs `?stream=json` could toggle it, or it could automatically use SSE for certain endpoints dedicated to SSE. We will clarify this in documentation. Internally, the code difference is minor – it’s mostly how we format the output.  
- *Non-Streaming (Standard) Support:* Of course, not all requests will be streamed. The gateway must also handle normal request/response cycles (where the client waits for the full result). So each endpoint will support both synchronous complete responses and streaming. Possibly the client indicates this via a parameter. If `stream=false` (default), the gateway will call the upstream and wait for the full response before returning to the client. If `stream=true`, it will initiate streaming. In some cases, the choice might also depend on the upstream: if the upstream supports streaming, we can offer it; if not, we can only do full responses. This detail will be managed per provider in the implementation (capabilities of each provider integration).  
- *Robustness in Streaming:* While streaming, if an error occurs mid-way (e.g., upstream times out after partially sending data), the gateway should handle it gracefully. Ideally, send an SSE event indicating an error or terminate the stream properly so the client isn’t left hanging indefinitely. We might enclose error info in a final event or set a flag like `event: error`. Similarly, if the client disconnects mid-stream (cancels the request), our server should handle the cancellation (FastAPI typically cancels the handler coroutine) – we should ensure any upstream connection is also cancelled to not waste resources.  
- *Testing SSE:* Special consideration will be given to ensure SSE endpoints conform to spec (e.g., sending the correct `:` comments and not including content-length). We will test with web clients to confirm that events are received as expected.  

**5. Integration with Arize Phoenix (Tracing & Prompt Logging):** The gateway will incorporate functionality to log each request (and its prompt data) to Arize Phoenix for tracing and analysis. This is both a functional and operational requirement: from the user’s perspective (developers and ML engineers), every call through the gateway will generate trace data they can inspect later. Key aspects:  
- *OpenTelemetry Instrumentation:* The gateway will utilize OpenTelemetry to create a **trace** for each request ([OpenTelemetry FastAPI monitoring | Uptrace](https://uptrace.dev/guides/opentelemetry-fastapi#:~:text=OpenTelemetry%20allows%20you%20to%20trace,issues%20quickly%20and%20streamline%20troubleshooting)). A trace consists of one or more spans; at minimum, we’ll have a span covering the entire request handling. If the request involves multiple steps (like calling two providers), we may create sub-spans for those. We will initialize OTel at startup (setting up a tracer provider, and an exporter that sends data to Arize’s collector). Arize Phoenix supports receiving traces via OTLP (OpenTelemetry Protocol) and has first-class support for many frameworks and SDKs ([Overview: Tracing | Phoenix](https://docs.arize.com/phoenix/tracing/llm-traces#:~:text=specific%20LLM%20vendor%20or%20framework,%28Python%2C%20Javascript%2C%20etc)), meaning our integration should work out-of-the-box with standard OTel. We’ll likely use Arize’s provided OTel wrapper (`arize-phoenix-otel`) for convenience ([Setup using Phoenix OTEL | Phoenix](https://docs.arize.com/phoenix/tracing/how-to-tracing/setup-tracing/setup-using-phoenix-otel#:~:text=Quickstart%3A%20)), which respects environment variables for endpoint and API keys.  
- *Span Attributes (Prompt Logging):* Within each trace, we will log **prompt details** and other important metadata. For an LLM request, the “prompt” is the input text or parameters sent to the model. We will attach this as attributes on the span (for example, `prompt.text`, `prompt.tokens` if token count is known, `model.name`, `provider` etc.). Phoenix has defined semantic conventions for prompt data and LLM parameters ([Overview: Tracing | Phoenix](https://docs.arize.com/phoenix/tracing/llm-traces#:~:text=temperature%20and%20system%20prompts%2C%20to,ensure%20optimal%20configuration%20and%20debugging)), so we will follow those as much as possible – e.g., using attribute keys like `llm.prompt_template`, `llm.prompt_variables`, etc., which Phoenix can recognize and display. By “loading prompts” into the tracing system, we enable Phoenix to show the exact prompts that were sent to the model for each request, which is invaluable for debugging and improving prompts. (If there are privacy concerns, we could choose to mask or hash parts of prompts, but assuming this is for internal analysis, we’ll log them in full).  
- *Performance Metrics:* Beyond prompts, the tracing will capture performance metrics. Each span records a duration (time taken to serve the request). Phoenix can aggregate these to show average latency per endpoint or provider ([Overview: Tracing | Phoenix](https://docs.arize.com/phoenix/tracing/llm-traces#:~:text=,to%20optimize%20performance%20and%20responsiveness)). If an upstream call is slow, having a child span for it will pinpoint that. We’ll also capture the size of responses or number of tokens in/out (if upstream provides that info, e.g., OpenAI returns token usage in the response). Logging token counts allows Phoenix to show breakdowns of token usage per call ([Overview: Tracing | Phoenix](https://docs.arize.com/phoenix/tracing/llm-traces#:~:text=,the%20most%20expensive%20LLM%20invocations)), helping identify cost drivers.  
- *Error Tracing:* In case of errors (exceptions, timeouts), we will ensure those are recorded in the trace. This might involve marking spans with an error status and attaching the error message. Phoenix can then let us see error rates and details for troubleshooting. For example, if a provider returns a rate-limit error, we capture that as a runtime exception event in the span ([Overview: Tracing | Phoenix](https://docs.arize.com/phoenix/tracing/llm-traces#:~:text=,the%20most%20expensive%20LLM%20invocations)).  
- *Project and Session Info:* Arize Phoenix has the concept of projects and sessions for grouping traces. We will configure the tracer to label traces by project if applicable. For instance, traces coming from a “project endpoint” can be tagged with that project’s name or ID. This way, in Phoenix UI one can filter traces by project. (The `PHOENIX_PROJECT_NAME` environment variable or parameter can be used to set a default project for all traces ([Setup using Phoenix OTEL | Phoenix](https://docs.arize.com/phoenix/tracing/how-to-tracing/setup-tracing/setup-using-phoenix-otel#:~:text=from%20phoenix,all%20installed%20OpenInference%20instrumentors)), and we might programmatically override it per request if needed). Likewise, if we want to group a sequence of related calls into a session (e.g., a multi-turn conversation), Phoenix supports session grouping – but that may be outside our scope unless the gateway orchestrates multi-turn interactions.  
- *Verification:* We will test the integration by sending some sample requests through the gateway and ensuring they appear in the Arize Phoenix dashboard with the correct data. The system should clearly document how to configure the Phoenix integration (e.g., setting environment variables: `PHOENIX_API_KEY` for authentication to the Arize platform, `PHOENIX_COLLECTOR_ENDPOINT` if using a self-hosted collector, etc., as per Arize docs ([Setup using Phoenix OTEL | Phoenix](https://docs.arize.com/phoenix/tracing/how-to-tracing/setup-tracing/setup-using-phoenix-otel#:~:text=These%20defaults%20are%20aware%20of,have%20set%20to%20configure%20Phoenix)) ([Setup using Phoenix OTEL | Phoenix](https://docs.arize.com/phoenix/tracing/how-to-tracing/setup-tracing/setup-using-phoenix-otel#:~:text=Using%20environment%20variables))). This will allow operators to enable/disable or redirect traces easily.  

**6. Extensible Tooling and MCP Support:** The gateway should be built with an **extensible architecture** to allow adding new features or integrations over time. One anticipated integration is support for **MCP (Model Context Protocol)**, so the design must not only accommodate current needs but also future expansions:  
- *Modular Codebase:* We will organize the code into modules (or packages) for different concerns – e.g., auth, provider clients, project handlers, observability, etc. New modules can be added without modifying the core. For example, if a new provider “Provider C” comes along, we add a `provider_c.py` integration and mount its routes. The gateway could even dynamically load providers based on config. This modular approach also applies to cross-cutting tools: if, say, we want to integrate a new logging service, we can implement it as a dependency or middleware plugin.  
- *Plugin Interface:* We might define a generic interface or use dependency injection for **tools**. For instance, a “tool” could be something that intercepts requests or responses for additional processing (like a censorship filter, analytics logger, etc.). The gateway could provide hook points (similar to middleware events or FastAPI dependencies) where such tools can be invoked. We will clearly document these extension points so that future developers know how to plug in functionality.  
- *MCP Integration:* **Model Context Protocol (MCP)** is an open standard that enables connecting language models to external data sources and tools in a uniform way ([Model Context Protocol - Arize AI](https://arize.com/blog/model-context-protocol/#:~:text=MCP%20is%20a%20universal%2C%20open,AI%20and%20any%20external%20functionality)). While not fully specified in this brief, we anticipate needing to either communicate with an MCP server or host one. To prepare for this:
  - The gateway could reserve a set of endpoints or a sub-application for MCP interactions. In the simplest case, if an external MCP server needs to be called, the gateway can call it like any other upstream API (making our gateway an MCP client). However, MCP might require persistent connections or streaming of context. 
  - Another scenario is embedding an MCP server within our gateway. Some community implementations run MCP servers that communicate via standard I/O or SSE. We’ve learned that **SSE is a recommended approach to integrate an MCP server into a FastAPI app** ([Building a Server-Sent Events (SSE) MCP Server with FastAPI](https://www.ragie.ai/blog/building-a-server-sent-events-sse-mcp-server-with-fastapi#:~:text=The%20Model%20Context%20Protocol%20,support%20in%20your%20FastAPI%20application)). This means we could spin up an MCP handler that listens for SSE connections (from an AI agent, for example) and facilitates a conversation between the model and external tools accessible via our gateway. Our architecture’s existing SSE support will help in doing this. We would essentially add an MCP router that the model can connect to, register tools, and exchange messages. 
  - At this stage, because details are sparse, we assume MCP integration is a *future plugin*. The requirement is to **“support extensible tooling and something referred to as 'mcp'”** – we interpret this as ensuring the design is flexible enough to incorporate MCP when ready. Concretely, we will avoid hard-coding things that make such integration hard. For example, we won’t assume only HTTP request/response style interactions; our streaming design and route modularity mean we can accommodate an event-driven protocol like MCP. We can also plan a placeholder in the code (maybe a module or interface named `mcp_integration.py`) that will house MCP-related code once the use-case is clear. 
- *Use Case for MCP:* To give a sense, MCP might allow an LLM to ask the gateway for data (like “fetch customer record from database”) in the middle of a conversation. The gateway could have an MCP endpoint that the LLM calls (via the MCP protocol) to fetch that data from an external tool, then returns it to the LLM. Implementing this would involve the gateway acting as a broker between the LLM and various tools (some of which could be our upstream providers). Because our gateway already can call multiple providers (including possibly databases or other microservices if we integrate them as “providers”), it is well-suited to serve as the “brain” behind an MCP server. All we need is to adhere to the MCP message format and keep a live connection. Our extensibility aim is to make such additions possible with minimal friction.  

**7. Additional Functional Requirements:**  
- *Auto-Documentation:* Leverage FastAPI’s automatic documentation generation. All endpoints should include docstrings and use Pydantic models for request/response where appropriate so that the `/docs` endpoint (Swagger UI) accurately reflects the API. This makes it easier for client developers to understand how to use the gateway.  
- *Health Check Endpoint:* Provide a simple health check (e.g., `GET /health`) that returns a 200 OK with maybe version info. This will not require auth (so that load balancers or uptime monitors can use it freely). It should quickly verify that the app is up (and possibly that it can reach critical upstreams, though a deep upstream check might be optional).  
- *Rate Limiting (Future):* While not explicitly required, we note that in a production scenario an API gateway often includes rate limiting per key to prevent abuse. Our design should allow plugging in a rate-limit mechanism later (perhaps as a dependency that tracks requests per minute per key). We won’t implement it now, but we mention it to acknowledge the consideration.  
- *Testing Requirements:* The project should include unit and integration tests for core functionality. For example, tests for auth (making sure a request without key is rejected), tests for each provider integration (maybe using a mock upstream server to simulate provider responses), and tests for streaming (ensuring the streaming endpoints actually stream and don’t buffer entire response). This ensures the gateway works as expected and will aid future modifications.  

## Non-Functional Requirements  

**Performance & Scalability:** The gateway should handle high loads with low latency overhead. FastAPI’s asynchronous nature gives a strong foundation for performance – under the hood it uses the uvloop event loop which is very efficient. We expect the gateway’s added processing (auth checks, minor transformations) to be lightweight compared to the network I/O time of calling upstream APIs. Nevertheless, we will design and test for performance. Targets could be for example: able to handle, say, 1000 concurrent requests (mix of regular and streaming) on a single instance without excessive latency. Because the service is containerized, scaling out horizontally is the primary strategy for handling increased load – we can run multiple containers behind a load balancer. Each instance should remain stateless (no session stickiness needed except possibly for SSE continuity, but any instance can handle any request generally). We should ensure that the container has enough CPU and memory; asynchronous I/O can handle many tasks but is still bound by CPU if we do expensive processing. If needed, we could also leverage multi-processing by running Uvicorn with multiple worker processes (though each FastAPI instance then is separate and doesn’t share data, which is fine due to stateless design). Our aim is to keep the overhead per request minimal (auth lookup O(1), a few dictionary transformations, etc.). Logging and tracing do add overhead, but we can configure the tracing to batch and send asynchronously so it doesn’t block the main flow. In summary, the gateway should introduce only a small constant latency on top of the upstream API latency (ideally < 50ms overhead for non-stream calls).  

**Reliability & Fault Tolerance:** The gateway needs to be reliable in the face of various failure modes. It should not crash if one upstream service is down or returns malformed data; instead, it should handle errors gracefully and return a controlled error response. We will implement global exception handlers in FastAPI to catch unhandled exceptions and turn them into 500 responses (with internal logging for debugging). For upstream timeouts or connection errors, we handle those as mentioned (e.g., return 504 Gateway Timeout or similar). The gateway itself should have high availability – running multiple instances ensures that if one instance crashes or restarts (e.g., during a deployment), others can continue serving traffic. We will also leverage Docker’s capabilities (and orchestrator’s, if in Kubernetes) to auto-restart or replace the container if it fails. In terms of code reliability, thorough testing will help. We should also use type hints and possibly linters to catch bugs early. Another aspect is **idempotency** for certain requests: since this is mostly a pass-through, we assume calls to upstream are not idempotent by nature (each call might generate a new completion, etc.), so the gateway doesn’t try to deduplicate or retry requests on its own (except maybe a retry for network glitch). If required, we could implement limited retry logic for transient errors to upstreams (but carefully, to not double-execute non-idempotent operations; perhaps only for GET-like retrieval operations).  

**Security:** Besides API key auth, other security considerations include:  
- *Transport Security:* We will serve the gateway over HTTPS in production (likely terminating SSL at a reverse proxy or load balancer). Within the container, it’s HTTP, but external traffic should be encrypted. That is more of a deployment detail but worth noting.  
- *Secrets Management:* The gateway will need to store secrets like upstream API keys (for calling external providers) and the list of valid client API keys. We must handle these carefully: they will be provided via environment variables or a secure config volume, not baked into the image. The code will ensure not to log secrets. We will also avoid exposing any sensitive info in responses. For example, if an upstream returns an error that includes an internal stack trace or key, the gateway should sanitize that out.  
- *CORS (if needed):* If this gateway is accessed from browser clients (and not just server-to-server), we might need to enable CORS for certain domains. FastAPI makes it easy to configure CORS middleware. We will set that up as required (it might not be open to all origins by default for security).  
- *DDOS Mitigation:* While full DDOS protection might be handled at a higher level (cloud firewall, etc.), the gateway should at least avoid heavy operations that could be exploited. Rate limiting (as mentioned) is a possible future addition. Also, since prompt inputs might be large (some models allow very long prompts), we should set a maximum payload size to prevent a malicious client from sending a huge request body and consuming memory. We can either rely on server/proxy limits or implement a check on content length.  
- *Dependency Security:* We will pin dependency versions and keep them updated to avoid known vulnerabilities. Docker image will be based on a slim Python base to reduce attack surface.  

**Observability & Monitoring:** We’ve covered tracing in functional reqs. In addition, general observability includes logging and metrics:  
- *Logging:* All major actions should be logged at appropriate levels (info for routine ops, warning for suspicious, error for failures). In production, these logs can be aggregated by whichever logging system is in place. We will ensure logs are structured (JSON log lines or at least consistent format) to aid parsing. No sensitive info in logs (like full API keys or full prompts if not necessary – maybe log prompt excerpts or IDs instead).  
- *Metrics:* We may integrate a metrics endpoint (like exposing Prometheus metrics). If using `uvicorn`, it could have built-in metrics for request count, etc., or we add a library like `prometheus-client` to track counts (requests by endpoint, errors by type, etc.). Even if not initially asked, having basic metrics (QPS, error rate, memory usage) will be useful. Docker/K8s can monitor CPU/memory at container level.  
- *Distributed Tracing:* (Already Arize Phoenix integration). If needed for other monitoring (like Jaeger or Zipkin), the OpenTelemetry setup can be configured to export to multiple backends. Our design allows swapping or adding exporters (OpenTelemetry’s benefit is vendor-neutral instrumentation ([OpenTelemetry FastAPI monitoring | Uptrace](https://uptrace.dev/guides/opentelemetry-fastapi#:~:text=OpenTelemetry%20specifies%20how%20to%20collect,vendors%20without%20changing%20the%20instrumentation)), so we could send traces to Arize and another backend in parallel if needed without changing the code instrumentation).  

**Maintainability & Extensibility:** The code should be clean and modular, following SOLID principles where possible. Using FastAPI means we can take advantage of pydantic for data models – making the code self-documenting and reducing boilerplate parsing. We will maintain separation of concerns: routing logic vs business logic vs integration details. Documentation will be provided (in-code via docstrings and an external README or developer guide explaining how to add a new provider or project route). This project brief itself will serve as a reference for the architecture and design decisions, helping future maintainers understand why things were done a certain way. We also note that the tech stack (FastAPI, Docker, OTel) is chosen for being widely adopted and having community support, which aids maintainability (it’s easier to find developers with these skills and lots of examples online for similar problems).  

**Testability:** The design should allow components to be tested in isolation. For instance, the provider client modules can be unit-tested by mocking the HTTP calls. The auth logic can be tested with dummy keys. We can run the FastAPI app in a test client to simulate calls and verify responses. By structuring code into small functions and classes, we make it more testable. Ensuring deterministic behavior (for example, avoid depending on global state as much as possible) will make tests reliable. 

**Compliance and Privacy:** If the gateway handles user prompts and possibly personal data, we should consider privacy. All data sent to upstream is the responsibility of that upstream’s compliance as well, but we as gateway should not log or expose data unnecessarily. If this system will be used in production handling user content, we would incorporate data encryption at rest for logs/traces if required and possibly provide a way to purge sensitive traces from Phoenix after analysis (that’s more on Phoenix’s side, which likely has data retention settings ([Overview: Tracing | Phoenix](https://docs.arize.com/phoenix/tracing/llm-traces#:~:text=,Data%20Retention))). Since this is an internal tool, we assume compliance requirements (like GDPR) are minimal, but it’s good practice to allow deletion of data on request and not to retain sensitive data longer than needed.

## Integration Details  

**Arize Phoenix (Tracing) Integration:** The integration with Arize Phoenix is a centerpiece for observability. We will use the Arize Phoenix OpenTelemetry SDK to send traces of each request to the Phoenix platform. Here’s how this will be set up and used in practice: 

- *Initialization:* In the application startup code, we’ll call `phoenix.otel.register()` (from the `arize-phoenix-otel` package) to configure OpenTelemetry ([Setup using Phoenix OTEL | Phoenix](https://docs.arize.com/phoenix/tracing/how-to-tracing/setup-tracing/setup-using-phoenix-otel#:~:text=Quickstart%3A%20)). We will supply the project name and possibly batching settings. For example: `register(project_name="MyGatewayProject", batch=True, auto_instrument=True)` which sets up a global tracer provider. The API key for Phoenix will be picked up from the `PHOENIX_API_KEY` env var, and the collector endpoint from `PHOENIX_COLLECTOR_ENDPOINT` if specified (otherwise it might default to Arize’s cloud endpoint) ([Setup using Phoenix OTEL | Phoenix](https://docs.arize.com/phoenix/tracing/how-to-tracing/setup-tracing/setup-using-phoenix-otel#:~:text=These%20defaults%20are%20aware%20of,have%20set%20to%20configure%20Phoenix)) ([Setup using Phoenix OTEL | Phoenix](https://docs.arize.com/phoenix/tracing/how-to-tracing/setup-tracing/setup-using-phoenix-otel#:~:text=Using%20environment%20variables)). By doing this, we ensure all spans we create are sent to Phoenix with the proper authentication. 

- *Automatic vs Manual Instrumentation:* The `auto_instrument=True` flag indicates that the SDK will attempt to automatically instrument known libraries (like ASGI framework, HTTP clients, etc.). This means we might get a span for the FastAPI request handling automatically and spans for outgoing HTTP calls automatically (if using an instrumented client library). We will verify which parts are auto-instrumented. Regardless, we will likely add some manual instrumentation to enrich the trace with domain-specific info (prompts, etc.). We can use OpenTelemetry’s Python API (`trace.get_tracer()` and span methods) or Arize’s higher-level helpers. Arize Phoenix also supports **decorators** that can trace function calls automatically ([Phoenix MCP Server | Phoenix](https://docs.arize.com/phoenix/integrations/phoenix-mcp-server#:~:text=,56)), which we might use for simple cases. 

- *Tracing Each Route:* For each API route in FastAPI, we’ll ensure a trace span is active. If auto instrumentation is working, an incoming request might already have an active span (named after the HTTP route). If not, we can manually start one at the top of the endpoint function using `with tracer.start_as_current_span("span-name"):`. Within that span context, we add attributes:
  - The API endpoint name or operation (though Phoenix will also capture the route path).
  - The API key or client ID (maybe hash it, to identify which user made the call without exposing the actual key).
  - The provider being called (for provider endpoints) or project name (for project endpoints).
  - Any parameters of interest, especially prompts. For example, if the request JSON has a field “prompt” and “max_tokens”, we do `span.set_attribute("llm.prompt_template", prompt_text)` and `span.set_attribute("llm.parameters.max_tokens", max_tokens)`. Phoenix will then record these. In the Phoenix UI, one can inspect a trace and see the prompt that led to a given response, which is extremely useful for prompt debugging ([Overview: Tracing | Phoenix](https://docs.arize.com/phoenix/tracing/llm-traces#:~:text=temperature%20and%20system%20prompts%2C%20to,ensure%20optimal%20configuration%20and%20debugging)). This satisfies the “loading prompts” requirement – essentially we are logging the prompts into Phoenix. Phoenix even highlights prompt templates and variables separately, which we could take advantage of by marking static template vs dynamic variables if our system has those concepts.  
  - If the endpoint is streaming, we might mark an attribute like `streaming: true` or record an event at the start and end of stream.

- *Upstream Call Spans:* When the gateway calls an upstream API, we want that to appear as a child span under the request. If using an HTTP client that is OTel-instrumented (for example, `httpx` can be instrumented via OpenTelemetry instrumentation libraries), it may automatically create an “HTTP Client” span with attributes like HTTP method, URL, status code. If not, we can manually create a span around the call. E.g., `with tracer.start_as_current_span("ProviderA API call"):` then do the request. We’d set attributes like `http.url`, `http.status_code` so that it follows common semantic conventions. This child span will let us see how long the external call took, separate from our own processing. If the upstream itself provides any trace ID (some APIs might return a request-id), we could log that too for cross-reference, though not strictly needed.

- *Trace Export:* The Phoenix OTel integration by default uses a background thread/process to batch and send trace data. We set it to batch mode ([Setup using Phoenix OTEL | Phoenix](https://docs.arize.com/phoenix/tracing/how-to-tracing/setup-tracing/setup-using-phoenix-otel#:~:text=tracer_provider%20%3D%20register%28%20project_name%3D%22default%22%2C%20,all%20installed%20OpenInference%20instrumentors)) so as not to slow the request. Each completed span gets exported (maybe on a schedule or at span end). We should ensure the exporter is properly flushing on app shutdown to not lose data. If running Phoenix self-hosted locally, we’ll point `PHOENIX_COLLECTOR_ENDPOINT` to that address; if using cloud, the API key and default endpoint suffice. Phoenix being vendor-neutral means we could also simultaneously send these traces to another system like Jaeger if needed (though not required now).

- *Using Trace Data:* Once this integration is in place, the development and ML teams can use Phoenix to monitor the system. For example, they can trace a user request that went to a project endpoint and see it called Provider X then Provider Y, and how long each took. They can confirm that the prompt sent was as expected, and if the result was bad, they have the prompt for analysis. Over time, Phoenix can display aggregated metrics: e.g., average latency of each provider’s calls, distribution of prompt lengths, error frequencies, etc. This helps in optimizing the system (maybe a provider is too slow – consider switching or adding caching; or a certain prompt often fails – improve it). The project brief doesn’t require building a UI for this – we rely on Phoenix’s UI.

- *Data Retention & Privacy:* We note that prompts and possibly outputs will be stored in Phoenix. Depending on policy, we may configure data retention periods ([Overview: Tracing | Phoenix](https://docs.arize.com/phoenix/tracing/llm-traces#:~:text=,Data%20Retention)) or ensure sensitive data is handled appropriately. Phoenix likely allows tagging or filtering out certain data if needed. Integration with Phoenix also means we could take advantage of its other features in the future, like prompt evaluations or feedback loops, by logging additional info (Phoenix supports logging feedback on traces, etc., which we could integrate if we have human feedback on outputs, for example).

**MCP and Other Tool Integrations:** While the specifics of additional integrations are to be determined, we outline how the gateway could integrate with an external tool or protocol like MCP: 

- If integrating an **external MCP server** (one that follows Model Context Protocol), the gateway could act as a client. For instance, a project endpoint might receive a request from a conversational AI agent that needs extra context; it could forward that request to an MCP server which connects to various tools (databases, web search, etc.), then return the result to the agent via our gateway. In such a case, our gateway just needs to know how to contact the MCP server (host, API, auth). This would be just another provider integration (just that MCP might use SSE or websockets). We would implement that like any provider: an MCP client module managing the connection (if SSE, perhaps maintaining a connection and awaiting events).

- If hosting an **internal MCP server**: We could embed an MCP server library (there are reference implementations in various languages). The Ragie.ai blog by Bob Remeika (Anthropic’s MCP co-author) provides a guide on building an MCP server with FastAPI and SSE ([Building a Server-Sent Events (SSE) MCP Server with FastAPI](https://www.ragie.ai/blog/building-a-server-sent-events-sse-mcp-server-with-fastapi#:~:text=The%20Model%20Context%20Protocol%20,support%20in%20your%20FastAPI%20application)). We can follow a similar approach: run an MCP server instance in-process and expose endpoints for it. Typically, an MCP server might expose an SSE endpoint (for the model to subscribe to events) and a POST endpoint for the model to send commands or something along those lines. We would mount these under, say, `/mcp/*`. The internal MCP server would then call out to “tools” which we can implement as functions. Some of those tools might be the same as our providers (e.g., a “SearchTool” that actually calls an upstream search API via our code). In essence, the gateway itself could serve as the “tools backend” for the MCP, since we already have connectivity to various services. This integration requires careful design to not block the main app (but since SSE is async, it fits in). We would treat the MCP server as a special component that registers with the main app on startup.

- **Extensible Middleware:** Beyond MCP, if we want to integrate other tooling (like a caching layer, or an analytics pipeline), we’ll utilize FastAPI’s middleware or dependency injection system. For example, to add request caching, we could insert a middleware that checks if a request (or its prompt) has been recently seen and serve from cache. To integrate another observability tool like Langfuse or Datadog APM, we could add their specific middleware alongside Phoenix. The key is our design doesn’t preclude adding these; the FastAPI framework allows multiple middlewares and dependencies to coexist, so we can extend as needed.

- **Testing Integrations:** For any added integration, we will ensure it doesn’t break the core functionality. E.g., enabling MCP should not interfere with normal provider endpoints. This might involve namespacing routes carefully and testing with integration toggled on/off.

In summary, integration points are clearly defined: tracing integration at startup and per request (with Arize), authentication integration via dependency, provider integration via modular clients, and future tool integration via plugin-like modules or sub-apps. The project is structured to allow these pieces to evolve independently.

## Deployment Considerations  

**Docker Containerization:** The API Gateway will be packaged as a Docker container image to ensure consistency across environments. We will create a Dockerfile that starts from a lightweight base (such as `python:3.11-slim` or an Alpine image if compatible). The Dockerfile will: 
1. Copy the gateway application code and requirements. 
2. Install dependencies (FastAPI, Uvicorn, httpx, Arize Phoenix SDK, etc.). 
3. Set up a non-root user (for security) to run the app. 
4. Specify the command to run the FastAPI app, likely something like: `uvicorn main:app --host 0.0.0.0 --port 8000`. In production, we might use `uvicorn --workers N` or even Gunicorn with Uvicorn workers for more concurrency if needed (though for async IO, often 1 worker per CPU is enough).  

We’ll expose port 8000 (or 80) in the container for the app. The container can then be run on any Docker host or orchestrator. Containerization guarantees that all necessary libraries (including C libraries, etc.) are included, and the app runs the same in dev and prod. 

**Configuration via Environment:** Rather than hardcoding config in the image, we will use environment variables for things like: upstream API base URLs, upstream API keys, list of valid client API keys (if not using a persistent store), Arize Phoenix API key and endpoint, etc. This makes the container portable and secrets manageable via your deployment platform (e.g., Kubernetes secrets or Docker secrets). We will document all the required env vars and config in a README (for example: `PROVIDER1_URL`, `PROVIDER1_KEY`, `VALID_API_KEYS` (could be a comma-separated list), `PHOENIX_API_KEY`, etc.). The FastAPI app on startup will read these and configure the internal components accordingly. 

**Running in Production:** In a real deployment, we would put this container behind a reverse proxy or load balancer. For instance, in Kubernetes, you might have a Service and an Ingress or use an API Gateway like Kong/NGINX at the edge that forwards to our service. In cloud, you might run it on AWS ECS or similar. The important deployment considerations in those scenarios are:
- *Scaling:* We can replicate the container to many instances. Since it’s stateless, a simple round-robin load balancing is fine. We do have to ensure that streaming responses are handled properly by the LB (some load balancers have settings for streaming, but usually TCP stream is fine). 
- *Timeouts:* Configure any upstream load balancers to have sufficiently large timeouts for our endpoints because some requests (especially streaming ones or long LLM completions) might take a while. For example, if an LLM takes 2 minutes to generate a response and streams it, the LB shouldn’t cut off the connection at 30 seconds. We might need to tune these values (or send periodic data to keep it alive). 
- *Resource Limits:* In container settings, assign adequate CPU and memory. FastAPI with Uvicorn is quite efficient but if expecting heavy load, ensure it has CPU time. Memory usage should mostly depend on concurrency (each active request might have some memory overhead, and large prompt/response content will use memory until streamed out). We might estimate memory usage per connection and set a reasonable limit (e.g., if each stream can buffer up to, say, 1MB, and 1000 concurrent streams, that’s 1GB potentially, so maybe allocate a couple GB to be safe if expecting that scale). The container should also handle spike gracefully – maybe by queueing some requests if too many come at once (though async naturally queues in event loop).

**Reverse Proxy (Nginx) Configuration:** If using Nginx as a reverse proxy in front of the container (common for serving HTTPS, etc.), special config is needed for SSE as mentioned. We will ensure the deployment documentation includes instructions like: 
```
location /stream/ {
    proxy_pass http://gateway:8000/stream/;
    proxy_http_version 1.1;
    proxy_set_header Connection "";
    # Disable buffering for SSE
    proxy_buffering off;
    # Or set header 
    add_header X-Accel-Buffering no;
}
```
This aligns with recommendations to turn off proxy buffering for SSE endpoints ([For Server-Sent Events (SSE) what Nginx proxy configuration is appropriate? - Server Fault](https://serverfault.com/questions/801628/for-server-sent-events-sse-what-nginx-proxy-configuration-is-appropriate#:~:text=However%2C%20instead%20of%20turning%20,fastcgi)). Alternatively, as noted, our application can send `X-Accel-Buffering: no` itself in SSE responses (which we plan to do) to signal Nginx to not buffer ([For Server-Sent Events (SSE) what Nginx proxy configuration is appropriate? - Server Fault](https://serverfault.com/questions/801628/for-server-sent-events-sse-what-nginx-proxy-configuration-is-appropriate#:~:text=However%2C%20instead%20of%20turning%20,fastcgi)) ([For Server-Sent Events (SSE) what Nginx proxy configuration is appropriate? - Server Fault](https://serverfault.com/questions/801628/for-server-sent-events-sse-what-nginx-proxy-configuration-is-appropriate#:~:text=Content,no)). We will test SSE through Nginx to make sure events flow immediately.

**SSL and Networking:** The container will likely run in an environment where SSL is terminated. If we needed to serve SSL from FastAPI directly (not typical in Kubernetes, but maybe in a simpler setup), we’d have to mount certificates. But we assume an external terminator for simplicity. The container should be run in a secure network or behind firewall since it will have access to upstream keys. Outbound internet access is required for it to call external APIs (like OpenAI, etc.), so ensure the host network allows that (e.g., appropriate VPC settings).

**Continuous Deployment:** Setting up CI/CD to build the Docker image on updates is advisable. Each release would produce an image tagged (maybe with a version or commit hash). This ensures that deploying a new version is as simple as pulling the new image and restarting containers. Rollback is also easy by using the previous image. 

**Monitoring and Logging in Deployment:** In production, logs from the container (stdout/stderr) should be collected by the platform (like CloudWatch Logs, ELK stack, etc.). We will format logs in JSON or key-value to ease parsing. If a centralized tracing system is used (like Arize Phoenix cloud or self-hosted), ensure the container can reach that endpoint (open firewall on the port 4317 or 443 as needed for OTLP). We might run the Phoenix collector as a sidecar or separate service if self-hosting. The environment variables for Phoenix (API key, project name) must be provided in the container runtime.

**Environment Parity:** It’s good to have dev/staging environments mimic production. We can use Docker Compose to simulate the gateway and maybe mock upstream services and Phoenix collector for integration testing. The project deliverables should include a docker-compose.yml that can bring up the gateway (and perhaps an example upstream mock) for local testing. This will also aid QA to test streaming behavior easily.

**Rolling Updates:** Because some clients might maintain long SSE connections, deploying a new version (which restarts the container) can cut those connections. In a load-balanced setup, we can do a rolling update: take down one instance at a time while others continue serving. Clients may reconnect (SSE will auto-reconnect in browsers). We should handle a reconnect gracefully (idempotent or continuing where left off if possible, though that’s complex; at least, not crashing). We’ll document that during deployment, active streams may be disrupted and that’s expected; if higher availability for streams is needed, one could implement client-side retry logic or use a sticky session to avoid bouncing between instances mid-session.

**Docker Image Security:** Use minimal base image to reduce vulnerabilities. Possibly use Docker’s best practices: run as non-root, set read-only file system if possible (the app doesn’t need to write to disk except maybe temporary files, which can be in a tmpfs). We will not include any compilers or dev tools in the final image – using multi-stage build, we compile if needed then copy only runtime. This results in a smaller and safer image. 

**Maintenance and Operations:** We will provide documentation for operators on how to: update environment configs, rotate API keys (for both clients and upstreams), interpret the logs and traces, and scale the service. Since the gateway might be critical infrastructure, we should have alarms on certain conditions (like if upstream X is consistently failing, or if the gateway’s error rate goes above threshold, etc.). These can be set up in the monitoring system using the logs or traces. 

Finally, in deployment docs, we will highlight the dependency on upstreams: if an upstream is down, the gateway is partially degraded. Perhaps providing a fallback or caching can mitigate that (future improvement). But at minimum, operations should know that an upstream outage will show up as increased error responses from the gateway (which they can see in logs/Phoenix). 

To summarize deployment: a robust Dockerized service, configurable via env, scalable horizontally, with considerations for streaming through proxies and secure operations, will be delivered. This allows the development team to implement the gateway and the DevOps team to deploy and manage it with confidence. The combination of FastAPI + Docker is a proven approach for quickly deploying high-performance APIs, and many of the needed features (logging, monitoring, scaling) can be achieved with standard tools in this ecosystem ([Building a API Gateway with FastAPI: Rate Limiting, Authentication, and Observability | by Bryan Antoine | Medium](https://medium.com/@b.antoine.se/building-a-api-gateway-with-fastapi-rate-limiting-authentication-and-observability-7a0f658f14a8#:~:text=In%20today%E2%80%99s%20microservices,and%20distributed%20tracing%20with%20Jaeger)) ([Building a API Gateway with FastAPI: Rate Limiting, Authentication, and Observability | by Bryan Antoine | Medium](https://medium.com/@b.antoine.se/building-a-api-gateway-with-fastapi-rate-limiting-authentication-and-observability-7a0f658f14a8#:~:text=FastAPI%20stands%20out%20for%20its,also%20significantly%20reduce%20development%20time)). 

